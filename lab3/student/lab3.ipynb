{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](img/571_lab_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 3: Text classification and hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert-warning\">\n",
    "    \n",
    "## Instructions  \n",
    "rubric={mechanics}\n",
    "\n",
    "You will earn points for following these instructions and successfully submitting your work on Gradescope.  \n",
    "\n",
    "### Before you start  \n",
    "\n",
    "- Read the **[Use of Generative AI Policy](https://ubc-mds.github.io/policies/)**.\n",
    "  \n",
    "- Review the **[General Lab Instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)**.\n",
    "    \n",
    "- Check the **[MDS Rubrics](https://github.com/UBC-MDS/public/tree/master/rubric)** for grading criteria.\n",
    "\n",
    "### Before submitting  \n",
    "\n",
    "- **Run all cells** (‚ñ∂‚ñ∂ button) to ensure the notebook executes cleanly from top to bottom.\n",
    "\n",
    "  - Execution counts must start at **1** and be sequential.\n",
    "    \n",
    "  - Notebooks with missing outputs or errors may lose marks.\n",
    "    \n",
    "- **Include a clickable link to your GitHub repository** below this cell.\n",
    "\n",
    "- Make at least 3 commits to your GitHub repository and ensure it's up to date. If Gradescope becomes inaccessible, we'll grade the most recent GitHub version submitted before the deadline.\n",
    "\n",
    "- **Do not upload or push data files** used in this lab to GitHub or Gradescope. (A `.gitignore` is provided to prevent this.)  \n",
    "\n",
    "\n",
    "\n",
    "### Submitting on Gradescope  \n",
    "\n",
    "- Upload **only** your `.ipynb` file (with outputs shown) and any required output files. Do **not** submit extra files.\n",
    "  \n",
    "- If needed, refer to the [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/).  \n",
    "- If your notebook is too large to render, also upload a **Web PDF** or **HTML** version.  \n",
    "  - You can create one using **File $\\rightarrow$ Save and Export Notebook As**.  \n",
    "  - If you get an error when creating a PDF, try running the following commands in your lab directory:  \n",
    "\n",
    "    ```bash\n",
    "    conda install -c conda-forge nbconvert-playwright\n",
    "    jupyter nbconvert --to webpdf lab1.ipynb\n",
    "    ```  \n",
    "\n",
    "  - Ensure all outputs are visible in your PDF or HTML file; TAs cannot grade your work if outputs are missing.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR REPO LINK GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3247a4b883a670c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "In this lab, we'll focus on two things:\n",
    "1. Working with text data\n",
    "2. Hyperparameter optimization\n",
    "\n",
    "As this is a quiz week, this lab would be shorter than previous weeks' labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Introducing the dataset and EDA <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "Let's develop our own SMS spam classification system using Kaggle's [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset). The goal is to predict whether a specific text message is spam or not. \n",
    "\n",
    "Please download the CSV and save it as `spam.csv` locally under the `data` folder. \n",
    "\n",
    "> **Some text messages in this dataset may contain offensive language, a reflection of the realities on such platforms üòî. If you are sensitive to such content, please avoid reading the raw messages.** \n",
    "\n",
    "\n",
    "The starter code provided below assumes that the data file is saved as \"spam.csv\" and is located in the data directory of this lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df = pd.read_csv(\"data/spam.csv\", encoding=\"latin-1\")\n",
    "sms_df = sms_df.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\n",
    "sms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's start by splitting the data. This time, we'll keep both `X` and `y` in the training and test splits since we'll need them for our exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(sms_df, test_size=0.2, random_state=123)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple EDA\n",
    "\n",
    "Just as we examine distributions and summary statistics of features in tabular data during EDA, we can also perform exploratory analysis on text data. However, traditional EDA techniques don't directly apply to text. Here, we'll explore one simple hypothesis that the length of a message (for example, ‚ÄúOK‚Äù has a length of 2) may differ between spam and ham (non-spam) messages. Message length might even serve as a useful feature for classifying messages as spam or ham. Let's investigate this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a length column\n",
    "\n",
    "train_df[\"length\"] = train_df[\"sms\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary stats by class\n",
    "\n",
    "summary = (\n",
    "    train_df.groupby(\"target\")[\"length\"]\n",
    "    .agg([\"mean\", \"min\", \"max\"])\n",
    "    .rename(columns={\"mean\": \"avg_len\", \"min\": \"shortest_len\", \"max\": \"longest_len\"})\n",
    ")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shortest and longest spam and non-spam (ham) messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"spam\", \"ham\"]:\n",
    "    subset = train_df[train_df[\"target\"] == label]\n",
    "    shortest_msg = subset.loc[subset[\"length\"].idxmin(), \"sms\"]\n",
    "    longest_msg = subset.loc[subset[\"length\"].idxmax(), \"sms\"]\n",
    "    print(f\"\\n {label.upper()} MESSAGES\")\n",
    "    print(f\"Shortest ({summary.loc[label, 'shortest_len']} chars): {shortest_msg}\")\n",
    "    print(f\"Longest  ({summary.loc[label, 'longest_len']} chars): {longest_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 1.1 Discussion\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Describe any differences you observe in the average lengths of spam and ham messages. What might these differences suggest?  \n",
    "2. Suppose we want to train a supervised machine learning model using this data.  \n",
    "   - How would you encode the `sms` column?  \n",
    "   - Would one-hot encoding be appropriate for this type of data? Explain your reasoning.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 1.2 Word clouds\n",
    "rubric={viz}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Generate two word clouds: one for spam messages and another for ham (non-spam) messages from `train_df` using [the `wordcloud` package](https://github.com/amueller/word_cloud) . To install it in your course's conda environment, use:\n",
    "\n",
    "   ```conda install -c conda-forge wordcloud```\n",
    "\n",
    "2. Briefly discuss your observations. \n",
    "\n",
    "> ‚ö†Ô∏è Important: Avoid installing packages directly from the notebook. Leaving installation commands in the notebook may cause issues when Gradescope processes your submission.\n",
    "\n",
    "> Note: This package was not covered in our lectures. You may refer to [the documentation and examples](https://amueller.github.io/word_cloud/auto_examples/simple.html) to learn how to generate word clouds.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Bag-of-words encoding and model building\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build our models. Let's start by creating `X_train`, `y_train`, `X_test`, and `y_test` from the `train_df` and `test_df` defined above.\n",
    "\n",
    "Note that `X_train` here is a 1-D array (or `Series`) rather than a 2-D array or `DataFrame`. This is because, unlike most other transformers, `CountVectorizer` expects a one-dimensional sequence of text documents as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df[\"sms\"], train_df[\"target\"]\n",
    "X_test, y_test = test_df[\"sms\"], test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the dummy classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy_scores = cross_val_score(dummy, X_train, y_train)\n",
    "dummy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy model achieves an accuracy of 0.86, which might seem quite high at first. However, this is largely due to class imbalance in our dataset. There are many more ham messages (3,843) than spam messages (614). As a result, a model that simply predicts every message as ham would already achieve high accuracy, even though it performs poorly at identifying spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will be encoding the `sms` column with bag-of-words representation. Let's explore this representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv.get_feature_names_out()\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is rather small and the vocabulary size is < 8K. Now let's explore some words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:10]  # First 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[0::100]  # Every 100th word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[7600:]  # Last 82 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words are going to be features in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 2.1 `SVC` with default hyperparameters\n",
    "rubric={autograde}\n",
    "\n",
    "Above we created a `CountVectorizer` but if we want to carry out cross-validation without breaking the golden rule, we'll need to create a pipeline. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Build a pipeline with two steps\n",
    "    - Step 1: `CountVectorizer` with `stop_words='english'`\n",
    "    - Step 2: `SVC` with `random_state=123` and default hyperparameters.\n",
    "\n",
    "2. Show cross-validation scores in different folds and report the mean cross-validation score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svc_pipe = None\n",
    "mean_cv_svc = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_validate(svc_pipe, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 2.2 Running a cross-validation fold without sklearn tools \n",
    "rubric={autograde}\n",
    "\n",
    "Scikit-learn provides many convenient utilities such as `make_pipeline` and `cross_validate`, which simplify model training and evaluation. However, these high-level tools can sometimes make it harder to understand what's happening behind the scenes.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Manually compute the validation score for an SVC model on the first fold. That is, train on 80% and validate on the remaining 20% of the `X_train` data.\n",
    "    - Use the provided `CountVectorizer` and `SVC` objects along with `X_train` and `y_train`, splitting them further with `train_test_split(..., shuffle=False)`. \n",
    "    - Do not use Scikit-learn‚Äôs `Pipeline`, `cross_validate`, or `cross_val_score`.\n",
    "    - Save the resulting score in a variable called `fold_score`.\n",
    "    \n",
    "> üí° Note: You might notice slight differences between the score you obtained here and the first fold score from Section 2.1. This is because `cross_val_score` uses stratified sampling, ensuring that each training and validation split has roughly the same proportion of spam  and ham messages. In contrast, our manual split here may not preserve the original class balance as effectively.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline so we can access its individual steps\n",
    "svc_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Extract the fitted CountVectorizer and SVC objects from the pipeline which you should be using in this exercise.\n",
    "countvec_q_2_2 = svc_pipe[\"countvectorizer\"]\n",
    "svc_q_2_2 = svc_pipe[\"svc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fold_score = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fold_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Hyperparameter optimization\n",
    "<hr>\n",
    "\n",
    "Let's carry out hyperparameter optimization for SVC. For all the sub-parts of this question, we will be using our `svc_pipe` from the previous exercise.\n",
    "\n",
    "So far we have been writing loops to try a bunch of different hyperparameter values and pick the one with lowest cross-validation error. This operation is so common that `scikit-learn` has some [built-in classes](https://scikit-learn.org/stable/modules/grid_search.html) to do it for you. In this exercise, we will explore two such classes:\n",
    "\n",
    "1. [`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) \n",
    "2. [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Optimizing `C` for SVM RBF using [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "The code below carries out: \n",
    "\n",
    "* Hyperparameter search over `C` by sweeping the hyperparameter through the values $10^{-2}, 10^{-1}, 1, 10^{1}, 10^{2}$ using [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and 5-fold cross-validation\n",
    "* Plots training and cross-validation curves (mean train and mean cross-validation scores for different values of `C` from the `param_grid`) with `C` on the x-axis (use log scale) and scores on the y-axis.\n",
    "* Identifies the \"optimal\" hyperparameter value and its associated best cross-validation score from the search using the `best_params_` attribute of the `GridSearchCV` object.\n",
    "  \n",
    "A few tips about `GridSearchCV`: \n",
    "\n",
    "- The starter code below defines the parameter grid for `C` which you can pass to your `GridSearchCV`. Note the syntax `svc__C`. We have two steps in our pipeline and we can access the parameters of these steps using `__` to go deeper. So `svc__C` means `C` of `svc` step of the pipeline. \n",
    "- Setting `n_jobs=-1` should speed things up (if you have a multi-core processor).\n",
    "- Similar to `cross_validate`, you can pass `return_train_score=True` to your `GridSearchCV` object.\n",
    "- By default, after carrying out the search, `GridSearchCV` also refits a model with the entire training set with the best hyperparameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"svc__C\": 10.0 ** np.arange(-2, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [],
   "source": [
    "def plot_train_cv(param_vals, train_scores, cv_scores, xlabel, ylabel=\"accuracy\"):\n",
    "    plt.semilogx(param_vals, train_scores, label=\"train\")\n",
    "    plt.semilogx(param_vals, cv_scores, label=\"valid\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"We are carrying out %d 5-fold cross-validation experiments\"\n",
    "    % (np.prod(list(map(len, param_grid.values()))))\n",
    ")\n",
    "\n",
    "C_search = GridSearchCV(svc_pipe, param_grid, n_jobs=-1, cv=5, return_train_score=True)\n",
    "C_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [],
   "source": [
    "train_scores = C_search.cv_results_[\"mean_train_score\"]\n",
    "cv_scores = C_search.cv_results_[\"mean_test_score\"]\n",
    "plot_train_cv(param_grid[\"svc__C\"], train_scores, cv_scores, xlabel=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify best C\n",
    "best_C = C_search.best_params_[\"svc__C\"]\n",
    "best_C_score = C_search.best_score_\n",
    "print(\"Best hyperparameter values: %0.3f\" % (best_C))\n",
    "print(\"Best score: %0.3f\" % (best_C_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as `C` increases, the training score improves. It's also important to note that the gap between training and validation scores doesn't change much once `C` reaches 1.\n",
    "\n",
    "While we do see some improvement after tuning `C` to 10, the difference isn't substantial. So in this case, it might be better to stick with the simpler model using `C=1`, since it performs nearly as well without the added complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 3.1 `GridSearchCV`: Jointly optimizing `C`, `gamma`\n",
    "rubric={autograde}\n",
    "\n",
    "So far, we have optimized only one hyperparameter at a time. Now, let's jointly optimize the `C`, along with `gamma` using `GridSearchCV`. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Expand your grid search to cover the `gamma` hyperparameter in addition to `C`, sweeping the hyperparameter through values $10^{-2}, 10^{-1}, \\ldots, 10^{2}$ for both `C` and `gamma`.\n",
    "2. Store the following in the corresponding variables below:\n",
    "   - `param_grid`: your parameter grid\n",
    "   - `gamma_C_search`: the fitted grid search object\n",
    "   - `best_gamma_C`: a dictionary containing the best hyperparameters found by your grid search\n",
    "   - `best_gamma_C_score`: the mean cross-validation score corresponding to the best parameters\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = None  # Parameter grid for grid search\n",
    "gamma_C_search = None  # Grid search object\n",
    "best_gamma_C = None  # Best hyperparameter values (dictionary)\n",
    "best_gamma_C_score = None  # Mean CV score with best hyperparameters\n",
    "\n",
    "...\n",
    "\n",
    "print(\"Best hyperparameter values: \", best_gamma_C)\n",
    "print(\"Best score: %0.3f\" % best_gamma_C_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 3.2 `RandomizedSearchCV`: Jointly optimizing SVM RBF and `CountVectorizer` hyperparameters \n",
    "rubric={accuracy,quality}\n",
    "\n",
    "Just like estimators, transformers also have hyperparameters. In this exercise, you'll jointly optimize hyperparameters of `SVC` as well as `CountVectorizer` using `RandomizedSearchCV`. \n",
    "\n",
    "Unlike `GridSearchCV`, which exhaustively tests all combinations, `RandomizedSearchCV` samples a fixed number of random hyperparameter combinations from the specified distributions or grids.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Use [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) with `n_iter` of your choice to jointly optimize the following hyperparameters:\n",
    "    - `C` and `gamma` for the SVC (RBF kernel)\n",
    "    - `binary` and `max_features` for the `CountVectorizer`\n",
    "2. Choose reasonable ranges or probability distributions for the hyperparameters based on your judgment You may use either a parameter grid or continuous distributions.\n",
    "4. Display both:\n",
    "    - The best hyperparameter values found by the random search\n",
    "    - The best cross-validation score obtained by the random search\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_dist = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_search = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_search_best_params = None\n",
    "random_search_best_score = None\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_search_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_search_best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 3.3  `RandomizedSearchCV` and `GridSearchCV`\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Did the best `C` and `gamma` value you found in `RandomizedSearchCV` match what you got in section 3.1 using `GridSearchCV`? In a broader context, should these values necessarily align? Explain your reasoning.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Test results <a name=\"5\"></a>\n",
    "<hr>\n",
    "\n",
    "Now that we have done extensive hyperparameter search, it's time to try our best model on the test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 4.1 Report test scores\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Evaluate your top-performing model on both the training and test sets, then assign the results to the respective variables provided below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_cv_score = None\n",
    "train_score = None\n",
    "test_score = None\n",
    "\n",
    "...\n",
    "\n",
    "print(\"CV score: %0.3f\" % best_cv_score)\n",
    "print(\"Train score: %0.3f\" % train_score)\n",
    "print(\"Test score: %0.3f\" % test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 4.2 Discussion and evaluation\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. How does your test accuracy compare to your validation accuracy? Briefly comment. \n",
    "2. Try out your best model on a couple of fake spam and ham (non-spam) text messages. You can start with some sample messages given below.  \n",
    "3. Do you believe that this model is likely to identify spam messages in the wild with the observed accuracy? Briefly discuss.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "sample_text_msgs = [\n",
    "    \"\"\"Call 8375467843 immediately. URGENT!! \n",
    "                    You have a free gift waiting for you...\"\"\",  # potential spam\n",
    "    \"MDS is fun.\",  # potential non-spam\n",
    "    \"\"\"This is very URGENT!! This week the MDS team has decided that \n",
    "                      all of you get 100% on all assignments and quizzes for free. \n",
    "                      Visit https://bit.ly/2T15k6V to check your grades.\"\"\",  # potential spam\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Food for thought\n",
    "<hr>\n",
    "\n",
    "Each lab will have a few challenging questions. In some labs, I will be including challenging questions which lead to the material in the upcoming week. These are usually low-risk questions and will contribute to maximum 5% of the lab grade. The main purpose here is to challenge yourself or dig deeper in a particular area. When you start working on labs, attempt all other questions before moving to these questions. **If you are running out of time, please skip these questions.** \n",
    "\n",
    "We will be more strict with the marking of these questions. There might not be model answers. If you want to get full points in these questions, your answers need to\n",
    "- be thorough, thoughtful, and well-written\n",
    "- provide convincing justification and appropriate evidence for the claims you make \n",
    "- impress the reader of your lab with your understanding of the material, your analytical and critical reasoning skills, and your ability to think on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-game-on.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### (Challenging) 5.1\n",
    "rubric={reasoning}\n",
    "\n",
    "In the EDA we noticed that the average length of spam vs. non-spam messages is quite different yet we are not using this feature in our current model. In this exercise, you will incorporate this feature in the model. \n",
    "\n",
    "**Your tasks:**\n",
    "1. Add a new numeric feature called `sms_length` in the modeling which represents the length of text in characters, along with the bag of words features. \n",
    "2. What's the cross-validation accuracy of SVC with default hyperparameters and only `sms_length` feature? \n",
    "3. What's the cross-validation accuracy of SVC with default hyperparameters and `sms_length` and `CountVectorizer` features together? \n",
    "4. Discuss your observations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### (Challenging) 5.2\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "There are many software packages that automate hyperparameter tuning beyond `GridSearchCV` and `RandomizedSearchCV`. Here are some examples:\n",
    "\n",
    "**Tied to scikit-learn**\n",
    "- [hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn)\n",
    "- [auto-sklearn](https://github.com/automl/auto-sklearn)\n",
    "- [TPOT](https://github.com/rhiever/tpot) (genetic programming approach)\n",
    "\n",
    "**General-purpose optimizers**\n",
    "- [Optuna](https://optuna.org/)\n",
    "- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html)\n",
    "- [FLAML](https://microsoft.github.io/FLAML/)\n",
    "- [hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "- [SMAC](http://www.cs.ubc.ca/labs/beta/Projects/SMAC/)\n",
    "\n",
    "*(Note: Some older packages like MOE, hyperband, pybo, Spearmint, and BayesOpt are no longer actively maintained but are interesting historically.)*\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Pick one of the tools above and try it on the task from the previous question.  \n",
    "2. Briefly describe:\n",
    "   - What algorithm it uses for optimization  \n",
    "   - How it compares to `RandomizedSearchCV` in terms of ease of use and performance  \n",
    "3. Reflect on whether you would use it in practice and why.\n",
    "\n",
    "> üí° Tip: If runtime or installation is an issue, feel free to analyze the package's example notebooks instead of running a full experiment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting your assignment, please ensure you have followed all the steps in the **Instructions** section at the top.  \n",
    "\n",
    "### Submission checklist  \n",
    "\n",
    "- [ ] Restart the kernel and run all cells (‚ñ∂‚ñ∂ button)\n",
    "- [ ] Make at least three commits to your Github repository. \n",
    "- [ ] The `.ipynb` file runs without errors and shows all outputs.  \n",
    "- [ ] Only the `.ipynb` file and required output files are uploaded (no extra files).  \n",
    "- [ ] If the `.ipynb` file is too large to render on Gradescope, upload a Web PDF and/or HTML version as well.\n",
    "- [ ] Include the link to your lab GitHub repository below the instructions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Congratulations on finishing the homework. You made it through a challenging week! Well done üëèüëè! \n",
    "\n",
    "![](img/eva-well-done.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q2.1": {
     "name": "q2.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not svc_pipe is None, 'Please use the provided variable'\n>>> assert sha1(str(round(mean_cv_svc, 3)).encode('utf-8')).hexdigest() == '4346e547194de927e67a1f4f4e2ba5381b4c5311', 'Are you storing mean cross-validation score in mean_cv_svc?'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not fold_score is None, 'Are you using the correct variable?'\n>>> assert sha1(str(np.round(fold_score, 3)).encode('utf8')).hexdigest() == '5a18a87924a1faff708ddac91c052dbf60c7db78', \"The fold_score doesn't look right.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": [
      1,
      1,
      2,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not param_grid is None, 'Please use the provided variables'\n>>> assert set(param_grid.keys()) == {'svc__C', 'svc__gamma'}, 'Please check the hyperparameters to optimize'\n>>> assert np.float64(100.0) in np.float64(param_grid['svc__gamma']), 'Please check the gamma values'\n>>> assert np.prod(list(map(len, param_grid.values()))) == 25, 'The total number of experiments is incorrect'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert not gamma_C_search is None, 'Please use the provided variables'\n>>> assert isinstance(gamma_C_search, GridSearchCV)\n>>> assert len(list(gamma_C_search.get_params()['estimator'])) == 2, 'Please check your estimator and you may need to revisit 3.1'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert not best_gamma_C is None, 'Please use the provided variables'\n>>> assert sha1(str(best_gamma_C.get('svc__C')).encode('utf8')).hexdigest() == 'c23f7da7070511444ebc75875fd9d202b5dd13cf', 'The best C value seems incorrect'\n>>> assert sha1(str(best_gamma_C.get('svc__gamma')).encode('utf8')).hexdigest() == '180505679cfe0cca79bae51fdda0296b7cd9c493', 'The best gamma value seems incorrect'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert not best_gamma_C_score is None, 'Please use the provided variables'\n>>> assert np.isclose(round(best_gamma_C_score, 2), 0.98, atol=0.1), 'The score is incorrect'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.1": {
     "name": "q4.1",
     "points": 2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
