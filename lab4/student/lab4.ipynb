{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](img/571_lab_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert-warning\">\n",
    "    \n",
    "## Instructions  \n",
    "rubric={mechanics}\n",
    "\n",
    "You will earn points for following these instructions and successfully submitting your work on Gradescope.  \n",
    "\n",
    "### Before you start  \n",
    "\n",
    "- Read the **[Use of Generative AI Policy](https://ubc-mds.github.io/policies/)**.\n",
    "  \n",
    "- Review the **[General Lab Instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)**.\n",
    "    \n",
    "- Check the **[MDS Rubrics](https://github.com/UBC-MDS/public/tree/master/rubric)** for grading criteria.\n",
    "\n",
    "### Before submitting  \n",
    "\n",
    "- **Run all cells** (▶▶ button) to ensure the notebook executes cleanly from top to bottom.\n",
    "\n",
    "  - Execution counts must start at **1** and be sequential.\n",
    "    \n",
    "  - Notebooks with missing outputs or errors may lose marks.\n",
    "    \n",
    "- **Include a clickable link to your GitHub repository** below this cell.\n",
    "\n",
    "- Make at least 3 commits to your GitHub repository and ensure it's up to date. If Gradescope becomes inaccessible, we'll grade the most recent GitHub version submitted before the deadline.\n",
    "\n",
    "- **Do not upload or push data files** used in this lab to GitHub or Gradescope. (A `.gitignore` is provided to prevent this.)  \n",
    "\n",
    "\n",
    "\n",
    "### Submitting on Gradescope  \n",
    "\n",
    "- Upload **only** your `.ipynb` file (with outputs shown) and any required output files. Do **not** submit extra files.\n",
    "  \n",
    "- If needed, refer to the [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/).  \n",
    "- If your notebook is too large to render, also upload a **Web PDF** or **HTML** version.  \n",
    "  - You can create one using **File $\\rightarrow$ Save and Export Notebook As**.  \n",
    "  - If you get an error when creating a PDF, try running the following commands in your lab directory:  \n",
    "\n",
    "    ```bash\n",
    "    conda install -c conda-forge nbconvert-playwright\n",
    "    jupyter nbconvert --to webpdf lab1.ipynb\n",
    "    ```  \n",
    "\n",
    "  - Ensure all outputs are visible in your PDF or HTML file; TAs cannot grade your work if outputs are missing.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "YOUR REPO LINK GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1: Naive Bayes by hand\n",
    "<hr>\n",
    "\n",
    "Naive Bayes is commonly used for text classification. In the lecture, we explored its use for spam detection. Now we'll apply it to another text classification task called [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis). \n",
    "\n",
    "Consider the toy data provided, which contains 10 training examples. Each example has 4 binary features indicating the presence or absence of a word, as well as the sentiment associated with each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_toy_data = {\n",
    "    \"predictable\": [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],\n",
    "    \"fun\": [0, 1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "    \"pathetic\": [0, 0, 1, 0, 0, 1, 0, 1, 1, 1],\n",
    "    \"satire\": [0, 0, 0, 0, 1, 1, 0, 1, 0, 1],\n",
    "    \"target\": [\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "        \"positive\",\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "        \"negative\",\n",
    "        \"negative\",\n",
    "        \"negative\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "toy_df = pd.DataFrame(sentiment_toy_data)\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this information, you want to predict the target (positive or negative sentiment) for the following unseen test example: \n",
    "    \n",
    "$$x_{new} = \\begin{bmatrix}1 & 1 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "In the following sub-exercises, we'll do this step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fdd9499f3c15117",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 1.1 Class prior probabilities\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Compute the estimates of the class prior probabilities by hand.\n",
    "1. Calculate $p(\\text{positive})$ and store the result in a variable named `pos_prior`.\n",
    "1. Calculate $p(\\text{negative})$ and store the result in a variable named `neg_prior`.\n",
    "\n",
    "Simply compute the raw frequencies/proportions and assign them to the appropriate variables as fractions, such as 1/2. There's no need to show your calculation steps or provide explanations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pos_prior = None  # type: float\n",
    "neg_prior = None  # type: float\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-664e58efe2282b30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 1.2 Conditional probabilities\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Manually calculate the conditional probabilities needed by naive Bayes for the test example $x_{new}$. \n",
    "    $$x_{new} = \\begin{bmatrix}1 & 1 & 0 & 1\\end{bmatrix}$$\n",
    "2. Store the conditional probability values in the following variables. Each variable corresponds to the likelihood of a word's presence or absence given its class. For example, `fun1_pos` stands for $p(\\text{fun} = 1  \\mid \\text{positive})$ and `pathetic0_pos` represents $p(\\text{pathetic} = 0  \\mid \\text{positive})$. \n",
    "\n",
    "You do not need to show any work. Also, do not consider Laplace smoothing here, just compute the raw frequencies/proportions by hand and store them into the appropriate variables as fractions (e.g., 1/2).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "predictable1_pos = None  # p(predictable = 1 | positive), type: float\n",
    "predictable1_neg = None  # p(predictable = 1 | negative), type: float\n",
    "fun1_pos = None  # p(fun = 1 | positive), type: float\n",
    "fun1_neg = None  # p(fun = 1 | negative), type: float\n",
    "pathetic0_pos = None  # p(pathetic = 0 | positive), type: float\n",
    "pathetic0_neg = None  # p(pathetic = 0 | negative), type: float\n",
    "satire1_pos = None  # p(satire = 1 | positive), type: float\n",
    "satire1_neg = None  # p(satire = 1 | negative), type: float\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3967f7905c1907f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 1.3 Prediction\n",
    "rubric={reasoning}\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Based on the naive Bayes model and the probabilities you've estimated, what is the most probable label for the test example, $x_{new}$, given below: \"positive\" or \"negative\"? Please show your calculations. \n",
    "\n",
    "$$x_{new} = \\begin{bmatrix}1 & 1 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "> We are not expecting any code here. Compute the probabilities by hand and show your steps. You may typeset in LaTeX (preferred) or upload a clear photo/scan of handwritten work.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 1.4 Smoothing\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Suppose you are asked to predict sentiment of another test example $\\hat{x}_{new}$. What issues might arise when calculating $p(positive \\mid \\hat{x}_{new})$? How would you address these challenges? \n",
    "\n",
    "$$\\hat{x}_{new} = \\begin{bmatrix}0 & 1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "> You don't have to write any code here. Just explanation in a few sentences is enough.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_1.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "⚠️ Don't forget to <code>git commit</code>. Regular commits will help you track your progress!  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Exercise 2: Implementing `DummyClassifier`\n",
    "<hr>\n",
    "rubric={autograde}\n",
    "\n",
    "In this course, you will generally **not** be asked to implement machine learning algorithms (like logistic regression) from scratch (You will be doing it in DSCI 572). However, this exercise is an exception: you will implement the simplest possible classifier, `DummyClassifier`.\n",
    "\n",
    "As a reminder, `DummyClassifier` is meant as a baseline and is generally the worst possible \"model\" you could \"fit\" to a dataset. All it does is predict the most popular class in the training set. So if there are more 0s than 1s it predicts 0 every time, and if there are more 1s than 0s it predicts 1 every time. For `predict_proba` it looks at the frequencies in the training set, so if you have 30% 0's 70% 1's it predicts `[0.3 0.7]` every time. Thus, `fit` only looks at `y` (not `X`).\n",
    "\n",
    "Below you will find starter code for a class called `MyDummyClassifier`, which has methods `fit()`, `predict()`, `predict_proba()` and `score()`. Your task is to fill in those four functions. To get you started, I have given you a `return` statement in each case that returns the correct data type: `fit` returns nothing, `predict` returns an array whose size is the number of examples, `predict_proba` returns an array whose size is the number of examples $\\times$ 2, and `score` returns a number.\n",
    "\n",
    "The next code block has some tests you can use to assess whether your code is working. \n",
    "\n",
    "I suggest starting with `fit` and `predict`, and making sure those are working before moving on to `predict_proba`. For `predict_proba`, you should return the proportion of each class in the training data. Your `score` function should call your `predict` function. Again, you can compare with `DummyClassifier` using the code below.\n",
    "\n",
    "To simplify this question, you can assume **binary classification**, and furthermore that these classes are **encoded as 0 and 1**. In other words, you can assume that `y` contains only 0s and 1s. Scikit-learn's `DummyClassifier` works when you have more than two classes, and also works if the target values are encoded differently, for example as \"cat\", \"dog\", \"elephant\", etc.\n",
    "\n",
    "> _Hint: In Python, if you define a variable within a class method using `self.variable_name`, it become an instance variable, allowing you to access it from other methods within the same class._\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class MyDummyClassifier:\n",
    "    \"\"\"\n",
    "    A baseline classifier that predicts the most common class.\n",
    "    The predicted probabilities come from the relative frequencies\n",
    "    of the classes in the training data.\n",
    "\n",
    "    This implementation only works when y only contains 0's and 1's.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Dummy Classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Training data.\n",
    "        - y (array-like, shape (n_samples,)): Target labels (0's and 1's).\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # Replace with your code\n",
    "        ...\n",
    "        return None  # Replace with your code\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target labels for the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred (array-like, shape (n_samples,)): Predicted target labels.\n",
    "        \"\"\"\n",
    "        predictions = np.zeros(X.shape[0])  # initializing with all predictions set to 0\n",
    "        # Replace with your code\n",
    "        ...\n",
    "        return predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - probs (array-like, shape (n_samples, 2)): Predicted class probabilities.\n",
    "          Column 0 corresponds to class 0, and column 1 corresponds to class 1.\n",
    "        \"\"\"\n",
    "        probs = np.zeros((X.shape[0], 2))  # initializing all probabilities set to 0.\n",
    "        # Replace with your code\n",
    "        ...\n",
    "        return probs  # Replace with your code\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model on the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Input data.\n",
    "        - y (array-like, shape (n_samples,)): True target labels.\n",
    "\n",
    "        Returns:\n",
    "        - accuracy (float): Accuracy of the model on the input data.\n",
    "        \"\"\"\n",
    "        accuracy = None\n",
    "        # Replace with your code\n",
    "        ...\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# For testing, generate random data\n",
    "n_train = 101\n",
    "n_valid = 21\n",
    "d = 5\n",
    "X_train_dummy = np.random.randn(n_train, d)\n",
    "X_valid_dummy = np.random.randn(n_valid, d)\n",
    "y_train_dummy = np.random.randint(2, size=n_train)\n",
    "y_valid_dummy = np.random.randint(2, size=n_valid)\n",
    "\n",
    "my_dc = MyDummyClassifier()\n",
    "sk_dc = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "my_dc.fit(X_train_dummy, y_train_dummy)\n",
    "sk_dc.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "assert np.array_equal(my_dc.predict(X_train_dummy), sk_dc.predict(X_train_dummy))\n",
    "assert np.array_equal(my_dc.predict(X_valid_dummy), sk_dc.predict(X_valid_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "y_train_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    my_dc.predict_proba(X_train_dummy), sk_dc.predict_proba(X_train_dummy)\n",
    ")\n",
    "assert np.allclose(\n",
    "    my_dc.predict_proba(X_valid_dummy), sk_dc.predict_proba(X_valid_dummy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert np.isclose(\n",
    "    my_dc.score(X_train_dummy, y_train_dummy), sk_dc.score(X_train_dummy, y_train_dummy)\n",
    ")\n",
    "assert np.isclose(\n",
    "    my_dc.score(X_valid_dummy, y_valid_dummy), sk_dc.score(X_valid_dummy, y_valid_dummy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "⚠️ Don't forget to <code>git commit</code>. Regular commits will help you track your progress!  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Exercise 3: Classifying happy moments\n",
    "<hr>\n",
    "\n",
    "Let's end this course on a happy note! We will use [HappyDB](https://www.kaggle.com/ritresearch/happydb) corpus which contains about 100,000 happy moments classified into 7 categories: *affection, exercise, bonding, nature, leisure, achievement, enjoy_the_moment*. The data was crowd-sourced via [Amazon Mechanical Turk (MTurk)](https://www.mturk.com/). The ground truth label is not available for all examples, and in this lab, we'll only use the examples where ground truth is available (~15,000 examples). \n",
    "\n",
    "- Download the data from [here](https://www.kaggle.com/ritresearch/happydb).\n",
    "- Unzip the file and copy it in a folder called `data` under the lab directory.\n",
    "\n",
    "The code below reads the data CSV (assuming that it's present in the current directory as *`data/cleaned_hm.csv`*),  cleans it up a bit, and splits it into train and test splits. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_hm.csv\", index_col=0)\n",
    "sample_df = df.dropna()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXKa7qfQXYPD",
    "outputId": "8bbf5eeb-0151-4853-a49c-3876279bbeb7"
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df.rename(\n",
    "    columns={\"cleaned_hm\": \"moment\", \"ground_truth_category\": \"target\"}\n",
    ")\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(sample_df, test_size=0.3, random_state=123)\n",
    "X_train, y_train = train_df[\"moment\"], train_df[\"target\"]\n",
    "X_test, y_test = test_df[\"moment\"], test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful to understand the distribution of our target values, and identify potential label imbalances in our dataset. If substantial imbalance exists, the model tends to overfit to the majority class while learning very little from the minority class. In more extreme cases, it might even default to predicting only the majority class. \n",
    "\n",
    "By checking the frequency of each target label, we can also estimate the baseline performance—the accuracy of a simple model. `DummyClassifier` would achieve this by always predicting the most frequent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate a clear class imbalance. The \"nature\" and \"exercise\" categories are underrepresented, while \"affection\" appears most frequently, accounting for roughly 34% of the examples. Consequently, a `DummyClassifier` that always predicts \"affection\" would achieve a baseline accuracy of about 34%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 3.1 Different classifiers \n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "1. For each model in the `models` dictionary below, perform 5-fold cross-validation. Show the mean and standard deviation of the following metrics:\n",
    "    - `fit_time`\n",
    "    - `score_time`\n",
    "    - `test_score` (cross-validation score)\n",
    "    - `train_score` (training score)\n",
    "2. Store results in a pandas dataframe named `results_df`, where\n",
    "    - Each row corresponds to a model\n",
    "    - Each column shows mean and standard deviation of the metrics listed above.\n",
    "    - Column names should be: `fit_time`, `score_time`, `test_score`, and `train_score`.\n",
    "      \n",
    "Example table format: \n",
    "\n",
    "  | Model          | fit_time        | score_time      | test_score   | train_score  |\n",
    "  |----------------|-----------------|-----------------|--------------|--------------|\n",
    "  | dummy          |  0.085 ± 0.005  |  0.021 ± 0.003  |  0.343 ± 0.0 |  0.343 ± 0.0 | \n",
    "  | decision tree  |                 |                 |              |              |\n",
    "  | kNN            |                 |                 |              |              |\n",
    "  | RBF SVM        |                 |                 |              |              |\n",
    "\n",
    "> Use the `build_pipeline` function below to set up pipelines for different models, which uses `CountVectorizer(stop_words=\"english\")`. \n",
    "\n",
    "> You may reuse `mean_std_cross_val_scores` function from the lecture notes (with attribution). \n",
    "\n",
    "> ⏳ The code might take some time to run. Be patient.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"dummy\": DummyClassifier(random_state=123),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=123),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RBF SVM\": SVC(random_state=123),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=123),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def build_pipeline(model):\n",
    "    return make_pipeline(CountVectorizer(stop_words=\"english\"), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 3.2 Discussion\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Reflect on the results from Exercise 3.1. You may consider the following questions for discussion:\n",
    "    - Which models excel in this task? \n",
    "    - Among the best performing models, which one is the fastest one? \n",
    "    - Which model or models seem to suffer from overfitting?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 3.3 Hyperparameter optimization \n",
    "rubric={accuracy,quality}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Define a pipeline with \n",
    "    - `CountVectorizer(stop_words=\"english\")`\n",
    "    - `LogisitcRegression(max_iter=2000)`\n",
    "2. Use `RandomizedSearchCV` to jointly optimize `C` of logistic regression and `max_features` of `CountVectorizer`. You can choose a suitable range for hyperparameter value and number of iterations. Make sure your `RandomizedSearchCV` is called `random_search` for the autograder to work properly in Exercises 4 and 5.\n",
    "3. Show the top 3 models based on your random search, highlighting their training scores, CV scores, hyperparameter configuratiions, and fit times.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "random_search = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 3.4 Discussion\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Did the hyperparameter optimization make a difference? Are there significant disparities in the cross-validation scores among the top three models produced by your random search? Provide a brief analysis.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "⚠️ Don't forget to <code>git commit</code>. Regular commits will help you track your progress!  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Exercise 4: Interpreting features, test score, and final evaluation\n",
    "<hr>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 4.1 Most informative words\n",
    "rubric={accuracy,quality}\n",
    "\n",
    "One of the major advantages of linear models is their interpretable coefficients, which allow us to understand predictions based on feature importances. It is beneficial to take a look at what the model has learned, which feature has it prioritized. Professional data scientist often would use their domain knowledge or past experience to judge if the coefficients of each feature are reasonable. In this exercise, we'll explore these coefficients learned by our logistic regression model. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Using the best estimator from 3.3, identify\n",
    "    - the top 5 words that are positively associated and top 5 words which are negatively associated with the class \"affection\". \n",
    "    - the top 5 words that are positively associated and top 5 words which are negatively associated with the class \"exercise\". \n",
    "\n",
    "> The information you need is exposed by the `coef_` attribute of [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) object. Note that for multiclass classification, for each class, the model learns coefficients per feature for that class. \n",
    "\n",
    "> The vocabulary (i.e., the mapping from feature indices to actual words) can be obtained by calling `get_feature_names_out()` on the `CountVectorizer` object. \n",
    "\n",
    "> You can adapt the code from lecture notes for this task. Please include a brief attribution like \"Code adapted from Lecture 8.\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "positive5_affection = None  # list\n",
    "negative5_affection = None  # list\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "positive5_exercise = None  # list\n",
    "negative5_exercise = None  # list\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 4.2 Evaluation on test data\n",
    "rubric={autograde}\n",
    "\n",
    "Hopefully, the most informative words identified in the previous exercise made sense. Now, let's evaluate how well our best model performs on both the test set and some unseen examples.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Evaluate the best model found by `random_search` from Exercise 3.3 on the entire train set and test set. Store the results in the corresponding variables below.\n",
    "\n",
    "> Note: When `refit=True` (the default), [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) automatically retrains the best model on the full training set after finding the optimal parameters. Therefore, you can directly call `.score()` on the `random_search` object with your train and test data to get the respective scores. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "random_search_best_score = None\n",
    "train_score = None\n",
    "test_score = None\n",
    "\n",
    "...\n",
    "\n",
    "print(\"Random Search best model score: %0.3f\" % random_search_best_score)\n",
    "print(\"Train score on the full train set: %0.3f\" % train_score)\n",
    "print(\"Test score on the full test set: %0.3f\" % test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 4.3 Evaluation using probability scores\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Using this model, find the following moments in the test set \n",
    "\n",
    "1. where the model is most confident that the moment belongs to class \"achievement\" (i.e., an example with highest predicted probability for class \"achievement\")\n",
    "2. where the model is most confident that the moment belongs to class \"nature\" (i.e., an example with the highest predicted probability of being \"nature\")\n",
    "\n",
    "In each case, print out the moment and the associated probability score. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "achievement_prob = None  # numpy.float64\n",
    "achievement_msg = None  # str\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "nature_prob = None  # numpy.float64\n",
    "nature_msg = None  # str\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27b53d359d7a9441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### 4.4 Fake moments \n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Test the best model on some fake moments. Some examples are given below. Feel free to add moments to this list. \n",
    "2. Briefly note your observations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "test_moments = [\n",
    "    \"I just finished my last assignment!\",\n",
    "    \"On the weekend, I spent some quality time with my best friend.\",\n",
    "    \"Collaborating with peers and teaching team members is what makes MDS enjoyable!!\",\n",
    "    \"I went for a hike in the forest.\",\n",
    "    \"I did yoga this morning.\",\n",
    "    \"I am still breathing and I am alive!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b3135a33bcfabb93",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "⚠️ Don't forget to <code>git commit</code>. Regular commits will help you track your progress!  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Exercise 5: Food for thought\n",
    "<hr>\n",
    "\n",
    "Each lab will have a few challenging questions. In some labs, I will be including challenging questions which lead to the material in the upcoming week. These are usually low-risk questions and will contribute to maximum 5% of the lab grade. The main purpose here is to challenge yourself or dig deeper in a particular area. When you start working on labs, attempt all other questions before moving to these questions. If you are running out of time, please skip these questions. \n",
    "\n",
    "We will be more strict with the marking of these questions. There might not be model answers. If you want to get full points in these questions, your answers need to\n",
    "- be thorough, thoughtful, and well-written\n",
    "- provide convincing justification and appropriate evidence for the claims you make \n",
    "- impress the reader of your lab with your understanding of the material, your analytical and critical reasoning skills, and your ability to think on your own\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-game-on.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### (Challenging) Exercise 5.1\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Reflect on your journey through this course. Please identify and elaborate on at least three key concepts or experiences where you had an \"aha\" moment. How would you use the concepts learned in this course in your personal projects or how would you approach your past projects differently based on the insights gained in this course? We encourage you to dig deep and share your genuine reflections.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### (Challenging) Exercise 5.2\n",
    "rubric={reasoning}\n",
    "\n",
    "Machine learning has its own workflows and good habits, such as when to split the data, writing your code as clear pipelines, and tuning your models in a reproducible manner, to ensure the validity of your results. In this course, we not only learned how to use a number of machine learning methods but also some good habits as a machine learning practitioner. In this exercise, I would like you to review a couple of Kaggle notebooks of your choice for some of the most popular datasets we have explored in this class and assess their methodology. \n",
    "\n",
    "To get you started, here are a couple of example notebooks\n",
    "\n",
    "- [Example notebook 1](https://www.kaggle.com/code/gudisesaichand/spotify-song-attributes-eda-and-prediction/notebook)\n",
    "- [Example notebook 2](https://www.kaggle.com/code/anuragnayak03/adult-income-classification-using-all-classifiers)\n",
    "\n",
    "and some aspects you might want to examine:  \n",
    "\n",
    "- Are they splitting the data before EDA?\n",
    "- Are they carrying out cross-validation? \n",
    "- Is their code well-written, compact, and reproducible?\n",
    "- Do you trust their results? Why or why not?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### (Challenging) Exercise 5.3\n",
    "rubric={reasoning}\n",
    "\n",
    "We have been saying that one of the main benefits of naive Bayes is that it's fast and scalable. \n",
    "\n",
    "**Your tasks:**\n",
    "1. Try naive Bayes on a large dataset of your choice. Report the scores, `fit` and `score` times. Below are a couple of suggestions for the datasets.  \n",
    "    - [Amazon customer reviews](https://www.kaggle.com/datasets/bhavikardeshna/amazon-customerreviews-polarity?select=train.csv)\n",
    "    - [Yelp tip](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset/versions/6?select=yelp_tip.csv)\n",
    "\n",
    "> **For this question, please do not include the code in this notebook because we do not want this question to affect the autograder. Please create a separate Jupyter notebook in your GitHub repository and add a link to that notebook here.**\n",
    "\n",
    "> You are welcome to explore the [`partial_fit` method supported by naive Bayes](https://scikit-learn.org/0.15/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB.partial_fit), which is useful when the whole dataset is too big to fit in memory at once.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "⚠️ Don't forget to <code>git commit</code>. Regular commits will help you track your progress!  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before submitting your assignment, please ensure you have followed all the steps in the **Instructions** section at the top.  \n",
    "\n",
    "### Submission checklist  \n",
    "\n",
    "- [ ] Restart the kernel and run all cells (▶▶ button)\n",
    "- [ ] Make at least three commits to your Github repository. \n",
    "- [ ] The `.ipynb` file runs without errors and shows all outputs.  \n",
    "- [ ] Only the `.ipynb` file and required output files are uploaded (no extra files).  \n",
    "- [ ] If the `.ipynb` file is too large to render on Gradescope, upload a Web PDF and/or HTML version as well.\n",
    "- [ ] Include the link to your lab GitHub repository below the instructions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on finishing your last lab of the course! 👏👏👏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-congrats.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": [
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sha1(str(pos_prior).encode('utf8')).hexdigest() == '015b8dcbc023be1734f726ca9af614fbb5b87611', 'pos_prior is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(neg_prior).encode('utf8')).hexdigest() == 'fbccebab4a05483ba1b7ce5c12688ac8f69ec024', 'neg_prior is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": [
      0.5,
      0.5,
      0.5,
      0.5,
      0.5,
      0.5,
      0.5,
      0.5
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sha1(str(predictable1_pos).encode('utf8')).hexdigest() == '1b390cd54a0c0d4f27fa7adf23e3c45536e9f37c', 'predictable1_pos is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(predictable1_neg).encode('utf8')).hexdigest() == 'a1fca854f9cf2577cf14dd55c8c38dd79e4c4bf2', 'predictable1_neg is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(fun1_pos).encode('utf8')).hexdigest() == '0cf1aeac0372e10da230a32e74c9b6e68dca7342', 'fun1_pos is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(fun1_neg).encode('utf8')).hexdigest() == '34a2d78560e4fc44dd67495c6ced9688f42dca1d', 'fun1_neg is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(pathetic0_pos).encode('utf8')).hexdigest() == 'e8dc057d3346e56aed7cf252185dbe1fa6454411', 'pathetic0_pos is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(pathetic0_neg).encode('utf8')).hexdigest() == '34a2d78560e4fc44dd67495c6ced9688f42dca1d', 'pathetic0_neg is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(satire1_pos).encode('utf8')).hexdigest() == 'bdedc3fe4985c37a90969413100c6d0faccb1f1b', 'satire1_pos is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(satire1_neg).encode('utf8')).hexdigest() == '1b390cd54a0c0d4f27fa7adf23e3c45536e9f37c', 'satire1_neg is incorrect, see traceback above.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_train_dummy_tests = np.random.randn(51, 5)\n>>> y_train_dummy_tests = np.append(np.ones(26), np.zeros(25))\n>>> X_valid_dummy_tests = np.random.randn(11, 5)\n>>> y_valid_dummy_tests = np.append(np.zeros(7), np.ones(4))\n>>> test_dc = MyDummyClassifier()\n>>> test_dc.fit(X_train_dummy_tests, y_train_dummy_tests)\n>>> assert np.array_equal(test_dc.predict(X_train_dummy_tests), np.ones(51)), 'Tests for predict failed'\n>>> assert np.array_equal(test_dc.predict(X_valid_dummy_tests), np.ones(11)), 'Tests for predict failed'\n>>> predict_test = test_dc.predict_proba(X_train_dummy_tests)\n>>> assert np.all(predict_test == predict_test[0]), 'Tests for predict_proba failed'\n>>> assert np.allclose(predict_test[0], np.mean(test_dc.predict_proba(X_valid_dummy_tests), axis=0), [0.49, 0.51]), 'Tests for predict_proba failed'\n>>> assert np.isclose(test_dc.score(X_train_dummy_tests, y_train_dummy_tests), 0.51, atol=0.01), 'Tests for score failed'\n>>> assert np.isclose(test_dc.score(X_valid_dummy_tests, y_valid_dummy_tests), 0.364, atol=0.01), 'Tests for score failed'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_train_dummy_tests = np.random.randn(35, 5)\n>>> y_train_dummy_tests = np.append(np.ones(10), np.zeros(25))\n>>> X_valid_dummy_tests = np.random.randn(7, 5)\n>>> y_valid_dummy_tests = np.append(np.zeros(1), np.ones(6))\n>>> test_dc = MyDummyClassifier()\n>>> test_dc.fit(X_train_dummy_tests, y_train_dummy_tests)\n>>> assert np.all(test_dc.predict(X_train_dummy_tests) == 0), 'Tests for predict failed'\n>>> assert np.all(test_dc.predict(X_valid_dummy_tests) == 0), 'Tests for predict failed'\n>>> predict_test = test_dc.predict_proba(X_train_dummy_tests)\n>>> assert np.all(predict_test == predict_test[0]), 'Tests for predict_proba failed'\n>>> assert np.allclose(predict_test[0], np.mean(test_dc.predict_proba(X_valid_dummy_tests), axis=0), [0.714, 0.286]), 'Tests for predict_proba failed'\n>>> assert np.isclose(test_dc.score(X_train_dummy_tests, y_train_dummy_tests), 0.714, atol=0.01), 'Tests for score failed'\n>>> assert np.isclose(test_dc.score(X_valid_dummy_tests, y_valid_dummy_tests), 0.143, atol=0.01), 'Tests for score failed'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not results_df is None, 'Please use the provided variable'\n>>> assert len(results_df.columns) >= 4, 'Please report mean cross-validation scores, mean train scores, and mean fit and score times'\n>>> assert 'test_score' in results_df.columns, 'Please ensure there is a column called test_score'\n>>> assert 'train_score' in results_df.columns, 'Please ensure there is a column called train_score'\n>>> assert results_df.shape[0] == 6, 'Please report stats for all six models'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.2": {
     "name": "q4.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not random_search_best_score is None, 'Please use the provided variables'\n>>> assert np.isclose(random_search_best_score, 0.831, atol=0.03), 'You might want to optimize the model further'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert not train_score is None and (not test_score is None), 'Please use the provided variables'\n>>> assert np.isclose(train_score, 0.957, atol=0.03), 'You might want to optimize the model further'\n>>> assert np.isclose(test_score, 0.824, atol=0.03), 'You might want to optimize the model further'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.3": {
     "name": "q4.3",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not achievement_prob is None and (not achievement_msg is None), 'Please use the provided variables'\n>>> assert not nature_prob is None and (not nature_msg is None), 'Please use the provided variables'\n>>> assert achievement_prob > 0.995, 'The achievement_prob looks wrong.'\n>>> assert nature_prob > 0.9, 'The nature_prob looks wrong.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
